{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "我们将使用以下数据集进行决策树的构建。该数据集包括7个特征，以及一个标签“是否适合读博”，这些特征描述了适合读博的各种条件，如love doing research,I absolutely want to be a college professor等。\n",
    "\n",
    "请运行下面的代码来加载数据集。\n",
    "\n",
    "（防侵权说明）参考https://zhuanlan.zhihu.com/p/372884253，数据集来源GPT4，但构造的决策树不一定与参考内容完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read decision_tree_datasets.csv\n",
    "train_data = pd.read_csv('train_phd_data.csv')\n",
    "test_data = pd.read_csv('test_phd_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 决策树构建 (10 分)\n",
    "在这个部分，你将学习并完成决策树的构建。注意：不考虑剪枝，决策树构建停止条件是数据所有实例属于同一类或者特征不可再分（即每个特征值都一样）。\n",
    "\n",
    "我们采用信息增益率作为分类标准，同时也允许使用其他指标，如基尼系数。\n",
    "\n",
    "请完成以下函数：\n",
    "\n",
    "1. **计算数据的信息熵** `getInfoEntropy()`\n",
    "2. **根据选取的特征进行数据分割** `split_data()`\n",
    "3. **根据分类标准找到最优特征** `find_best_feature()`\n",
    "\n",
    "你可能会用到`pandas`库函数，请参考 [pandas官方文档](https://pandas.pydata.org/docs/reference/)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoEntropy(data):\n",
    "    ''' \n",
    "        calculate the information entropy of the data\n",
    "\n",
    "    Args:\n",
    "        data: the data set, the last column is the label, the other columns are the features\n",
    "\n",
    "    Returns:\n",
    "        Entropy: float, the information entropy of the data\n",
    "    '''\n",
    "    \n",
    "    Entropy = 0.0\n",
    "    \n",
    "    # TODO: 1. count the number of different labels samples (each class has how many samples) -- count_class\n",
    "    ## hint: use pd.value_counts() to count the number of different labels samples\n",
    "    ## hint: use data.iloc[:,-1] to get the last column of the data\n",
    "    count_class = data.iloc[:,-1].unique()\n",
    "    \n",
    "    # TODO: 2. calculate the number of data\n",
    "    data_count = data.iloc[:,-1].value_counts().values\n",
    "    \n",
    "    \n",
    "\n",
    "    for i in range(len(count_class)):\n",
    "        # TODO: 3. calculate the probability of each class\n",
    "        p = data_count[i]/data.shape[0]\n",
    "        \n",
    "        # 防止溢出，如果完全分类\n",
    "        if p*(1-p) != 0:\n",
    "        # TODO: 4. calculate the entropy of each class\n",
    "            Entropy += - p*np.log2(p)\n",
    "        else:\n",
    "            Entropy += 0\n",
    "    #print('当前数据集的信息熵为：',Entropy)\n",
    "    return Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8256265261578954\n"
     ]
    }
   ],
   "source": [
    "## test getInfoEntropy\n",
    "print(getInfoEntropy(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, column):\n",
    "    ''' \n",
    "        split the data set according to the feature column\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "            column: the feature column\n",
    "        Returns:\n",
    "            splt_datas: Series, the data set after splitting\n",
    "    '''\n",
    "    # 1. construct a Series to save the data set after splitting\n",
    "    splt_datas = pd.Series(dtype = 'float64')  \n",
    "    # 2. get the unique values of the feature column\n",
    "    str_values = data.iloc[:,column].unique()  \n",
    "    # 3. find the data set corresponding to each unique value\n",
    "    for i in range(len(str_values)):   \n",
    "        df = data.loc[data.iloc[:,column] == str_values[i]]\n",
    "\n",
    "        splt_datas[str(i)] = df\n",
    "    return splt_datas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_feature(data):\n",
    "\n",
    "    '''  \n",
    "        find the best feature to split the data set\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            best_feature: the best feature\n",
    "            best_Series: Series, the data set after splitting\n",
    "    '''\n",
    "    best_feature_index = 0    \n",
    "    baseEnt = getInfoEntropy(data)  \n",
    "    bestInfoGain_ratio = 0.0\n",
    "    numFeatures = data.shape[1] - 1   \n",
    "    InfoGain = 0.0 \n",
    "    \n",
    "    # Loop through each feature to calculate information gain ratio.\n",
    "    for i in range(numFeatures):\n",
    "        newEnt = 0.0\n",
    "        # avoid div 0 error\n",
    "        IV = 1e-5\n",
    "        # TODO: 1. split the data set according to the feature column\n",
    "        series = split_data(data,i)\n",
    "\n",
    "        # 2. calculate the information entropy of each data set, and calculate the weighted average information entropy\n",
    "        for j in range(len(series)):\n",
    "            df = series[j]\n",
    "            # TODO: 3. calculate the probability of each data set\n",
    "            p = getInfoEntropy(df)\n",
    "            # TODO: 4. calculate the weighted average information entropy\n",
    "            weight = df.shape[0]/data.shape[0]\n",
    "            newEnt += weight*p\n",
    "            # TODO: 5. calculate the entropy of class labels IV\n",
    "            IV += -weight*np.log2(weight)\n",
    "        \n",
    "        # TODO: 6. calculate the information gain \n",
    "        InfoGain = baseEnt-newEnt\n",
    "        \n",
    "        # TODO: 7. calculate the information gain ratio\n",
    "        InfoGain_ratio = InfoGain/IV\n",
    "        # print(u\"第%d个特征的信息增益率为：%.3f\" % (i, InfoGain_ratio))\n",
    "\n",
    "        if InfoGain_ratio > bestInfoGain_ratio:\n",
    "            bestInfoGain_ratio = InfoGain_ratio\n",
    "            best_feature_index = i\n",
    "            best_Series = series\n",
    "        \n",
    "    return data.columns[best_feature_index], best_Series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create decision tree\n",
    "def creat_Tree(data):\n",
    "\n",
    "    '''\n",
    "        create decision tree\n",
    "\n",
    "        Args:\n",
    "            data: the data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            Tree: dict, the decision tree\n",
    "    '''\n",
    "    # get the class labels of the data set\n",
    "    y_values = data.iloc[:,-1].unique()   \n",
    "\n",
    "    # TODO: 1. If there is only one class label, stop splitting and return the class label.\n",
    "    if len(y_values) == 1:\n",
    "        return y_values[0]\n",
    "    \n",
    "    # 2. Check if the value of each feature is the same. If so, return the class label with the most samples.\n",
    "    flag = 0\n",
    "    for i in range(data.shape[1]-1):   \n",
    "        if(len(data.iloc[:,i].unique()) != 1):\n",
    "            flag = 1\n",
    "            break\n",
    "    \n",
    "    # TODO: 3. If all features are identical, return the class label with the most samples.\n",
    "    if(flag == 0):\n",
    "        value_count = data.iloc[:,-1].value_counts()\n",
    "        return value_count.idmax()\n",
    "\n",
    "    # 4. TODO: Find the best feature to split the data set.\n",
    "    best_feature, best_Series = find_best_feature(data)\n",
    "\n",
    "    Tree = {best_feature:{}}\n",
    "    # 5. Build the tree recursively. \n",
    "    for j in range(len(best_Series)):    \n",
    "        split_data = best_Series.iloc[j]\n",
    "        \n",
    "        # read the value of the best feature\n",
    "        value = split_data.loc[:,best_feature].unique()[0]  \n",
    "\n",
    "        # delete the best feature \n",
    "        split_data = split_data.drop(best_feature, axis = 1) \n",
    "        \n",
    "        # TODO: 6. recursively call the function to build the tree\n",
    "        Tree[best_feature][value] = creat_Tree(split_data)\n",
    "        \n",
    "    return Tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tree = creat_Tree(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.png'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize decision tree\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def plot_tree(tree, parent_name, node_id=0, graph=None, edge_label=''):\n",
    "    \n",
    "    \n",
    "    if graph is None:\n",
    "        graph = Digraph(comment='Decision Tree')\n",
    "\n",
    "    \n",
    "    if not isinstance(tree, dict):\n",
    "        current_node_name = f'node{node_id}' \n",
    "        graph.node(current_node_name, label=str(tree))\n",
    "        graph.edge(parent_name, current_node_name, label=edge_label)\n",
    "        node_id += 1\n",
    "        return node_id\n",
    "    \n",
    "    for k, v in tree.items():\n",
    "        current_node_name = f'node{node_id}' \n",
    "        node_label = f'{k}' if isinstance(v, (str, int)) else k\n",
    "        graph.node(current_node_name, label=node_label)\n",
    "        graph.edge(parent_name, current_node_name, label=str(edge_label))\n",
    "\n",
    "        if isinstance(v, dict):\n",
    "            for key in v:\n",
    "                # 假设分支可以用 '0' 和 '1' 来区分\n",
    "                node_id += 1\n",
    "                node_id = plot_tree(v[key], current_node_name, node_id, graph, edge_label=str(key))\n",
    "                \n",
    "    return node_id\n",
    "\n",
    "# plot decision tree\n",
    "tree_graph = Digraph(comment='Decision Tree')\n",
    "plot_tree(Tree, 'Root', 0, tree_graph)\n",
    "tree_graph.render('decision_tree', format='png', cleanup=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classfiy test data\n",
    "def classify(tree, test_data):\n",
    "    ''' \n",
    "        classify test data\n",
    "\n",
    "        Args:\n",
    "            tree: dict, the decision tree\n",
    "            test_data: the test data set, the last column is the label, the other columns are the features\n",
    "        Returns:\n",
    "            class_label: the predicted class label\n",
    "    '''\n",
    "    ## get the checked feature of the decision tree\n",
    "    first_str = list(tree.keys())[0]\n",
    "    \n",
    "    ## get the index of the feature\n",
    "    feat_index = test_data.index.get_loc(first_str)\n",
    "\n",
    "    # get the value of the feature to be checked in the decision tree\n",
    "    value = test_data.iloc[feat_index]\n",
    "\n",
    "    # TODO: get the subtree corresponding to the value of the feature\n",
    "    subtree = tree[first_str][value]\n",
    "\n",
    "    # TODO: recursively call the function to classify the test data\n",
    "    # hint: if the value of the feature is not a dict, it means that the decision tree has reached the leaf node, and the value of the feature is the predicted class label\n",
    "    if type(value) != dict:\n",
    "        return value\n",
    "    else:\n",
    "        classify(subtree,test_data)\n",
    "\n",
    "def test(test_data, tree):\n",
    "    right_label = []\n",
    "    for i in range(len(test_data)):\n",
    "        sample = test_data.iloc[i]\n",
    "        \n",
    "        # whether the predicted class label is equal to the true class label\n",
    "        if classify(tree, sample) == sample[-1]:\n",
    "            # if equal, the predicted is 1\n",
    "            right_label.append(1)\n",
    "        else:\n",
    "            right_label.append(0)\n",
    "    \n",
    "    acc = sum(right_label) / len(right_label)\n",
    "    print('accuracy: ', acc)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7142857142857143\n",
      "accuracy:  0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "test(test_data, Tree)\n",
    "test(train_data, Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将进行一项小测试，目的在于评估您是否适合攻读博士学位。\n",
    "\n",
    "请注意！这仅仅是基于假设的模型，无法准确预测实际情况。请将其视为一次轻松的尝试，仅供娱乐之用，不要用其来替代对自身状况的思考与决策。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Congratulations! According to the model, you are likely to gain admission for Ph.D.\n"
     ]
    }
   ],
   "source": [
    "# You can input your profile to predict your phd admission result\n",
    "# input your profile about \"I love doing research,I absolutely want to be a college professor,Money is important to me,I can deal with extreme stress and competition,I am OK being with judged all the time,I need a clear target and immediate feedback,I work 9-5 Mon-Fri\"\n",
    "loving = input('Do you love research? (1/0)')\n",
    "professor = input('Do you want to be a professor? (1/0)')\n",
    "money = input('Is money important to you? (1/0)')\n",
    "stress = input('Can you deal with stress? (1/0)')\n",
    "judge = input('Can you deal with being judged all the time? (1/0)')\n",
    "feedback = input('Do you need a clear target and immediate feedback? (1/0)')\n",
    "work = input('Do you work 9-5 Mon-Fri? (1/0)')\n",
    "\n",
    "# Combine the user's responses into a single data frame\n",
    "test_data = pd.Series({\n",
    "    'I love doing research': int(loving),\n",
    "    'I absolutely want to be a college professor': int(professor),\n",
    "    'Money is important to me': int(money),\n",
    "    'I can deal with extreme stress and competition': int(stress),\n",
    "    'I am OK being with judged all the time': int(judge),\n",
    "    'I need a clear target and immediate feedback': int(feedback),\n",
    "    'I work 9-5 Mon-Fri': int(work)\n",
    "})\n",
    "\n",
    "# Use the decision tree to predict the result \n",
    "result = classify(Tree, test_data)\n",
    "\n",
    "# Print the result to the user\n",
    "if result == 1:\n",
    "    print(\"Congratulations! According to the model, you are likely to gain admission for Ph.D.\")\n",
    "elif result == 0: \n",
    "    print(\"Unfortunately, according to the model, you are unlikely to gain admission for Ph.D.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
