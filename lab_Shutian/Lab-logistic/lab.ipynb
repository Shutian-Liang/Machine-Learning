{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备工作\n",
    "### 环境准备\n",
    "\n",
    "请确保完成以下依赖包的安装，并且通过下面代码来导入与验证。运行成功后，你会看到一个新的窗口，其展示了一张空白的figure。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import Tuple, List\n",
    "\n",
    "# display the plot in a separate window\n",
    "%matplotlib tk\n",
    "\n",
    "np.random.seed(12)\n",
    "\n",
    "# create a figure and axis\n",
    "plt.ion()\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "plt.style.use('default')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集准备\n",
    "\n",
    "你将使用以下二维数据集来训练逻辑分类器，并观察随着训练的进行，线性分割面的变化。\n",
    "\n",
    "该数据集包含两个特征和一个标签，其中标签 $ y \\in \\{-1,1\\} $。\n",
    "\n",
    "请执行下面的代码以加载数据集并对其进行可视化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_generator import gen_2D_dataset\n",
    "\n",
    "x_train, y_train = gen_2D_dataset(100, 100, noise = 0)\n",
    "x_test, y_test = gen_2D_dataset(50, 50, noise = 0.7) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vis_util import visualize_2D_dataset, visualize_2D_border\n",
    "\n",
    "visualize_2D_dataset(x_train, y_train)\n",
    "visualize_2D_dataset(x_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 逻辑回归 (10 pts)\n",
    "\n",
    "在这一部分，你将学习并完成逻辑回归相关代码的编写与训练。\n",
    "\n",
    "在运行这部分代码之前，请确保你已经完成了 `logistics.py` 文件的代码补全。\n",
    "\n",
    "完成后，运行以下代码，你会看到一张figure来展示$||w||$，loss和决策边界的变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 136.14532709098182, w_module: 8.993435416843155\n",
      "iter: 10, loss: 0.35975963071766476, w_module: 24.24719620628085\n",
      "iter: 20, loss: 0.13118174150957573, w_module: 24.278731117586574\n",
      "iter: 30, loss: 0.08093654399070661, w_module: 24.297910118626493\n",
      "iter: 40, loss: 0.05906695316217071, w_module: 24.31222741327293\n",
      "iter: 50, loss: 0.047005791155310055, w_module: 24.323904395326306\n",
      "iter: 60, loss: 0.03950744604858232, w_module: 24.33393739688583\n",
      "iter: 70, loss: 0.034509407496293956, w_module: 24.342868008663853\n",
      "iter: 80, loss: 0.031030767114426334, w_module: 24.351025905003336\n",
      "iter: 90, loss: 0.028541821341288576, w_module: 24.35862694836148\n",
      "iter: 100, loss: 0.026728971724399628, w_module: 24.36581952662652\n",
      "iter: 110, loss: 0.025393239126046394, w_module: 24.372709013530546\n",
      "iter: 120, loss: 0.024401547545705372, w_module: 24.37937183936708\n",
      "iter: 130, loss: 0.023661333032306744, w_module: 24.38586417165755\n",
      "iter: 140, loss: 0.023106373306863794, w_module: 24.392227590096603\n",
      "iter: 150, loss: 0.022688383985559987, w_module: 24.398492975547658\n",
      "iter: 160, loss: 0.02237174614962187, w_module: 24.404683274144677\n",
      "iter: 170, loss: 0.02213002260252635, w_module: 24.410815514075097\n",
      "iter: 180, loss: 0.021943549541041798, w_module: 24.416902302412318\n",
      "iter: 190, loss: 0.021797710366113183, w_module: 24.42295294669923\n",
      "iter: 200, loss: 0.021681665970476372, w_module: 24.428974298611863\n",
      "iter: 210, loss: 0.021587405600297132, w_module: 24.434971388509453\n",
      "iter: 220, loss: 0.021509031581002994, w_module: 24.440947901445355\n",
      "iter: 230, loss: 0.02144221908912973, w_module: 24.44690653278724\n",
      "iter: 240, loss: 0.021383808819225532, w_module: 24.452849252624507\n",
      "iter: 250, loss: 0.02133150108285348, w_module: 24.458777501383974\n",
      "iter: 260, loss: 0.021283627272806128, w_module: 24.464692333858196\n",
      "iter: 270, loss: 0.021238980084631987, w_module: 24.470594524781443\n",
      "iter: 280, loss: 0.021196688091031108, w_module: 24.476484645913413\n",
      "iter: 290, loss: 0.021156123568289154, w_module: 24.482363122126852\n",
      "iter: 300, loss: 0.02111683508245457, w_module: 24.488230272100324\n",
      "iter: 310, loss: 0.021078498393336136, w_module: 24.49408633777171\n",
      "iter: 320, loss: 0.021040880831086652, w_module: 24.499931505617337\n",
      "iter: 330, loss: 0.021003815530346418, w_module: 24.505765922004244\n",
      "iter: 340, loss: 0.020967182844297282, w_module: 24.511589704256266\n",
      "iter: 350, loss: 0.020930896967963422, w_module: 24.517402948626824\n",
      "iter: 360, loss: 0.02089489632862762, w_module: 24.523205736042648\n",
      "iter: 370, loss: 0.020859136692849563, w_module: 24.528998136243203\n",
      "iter: 380, loss: 0.020823586228097126, w_module: 24.534780210766122\n",
      "iter: 390, loss: 0.02078822196807703, w_module: 24.540552015102836\n",
      "iter: 400, loss: 0.02075302728459182, w_module: 24.546313600257612\n",
      "iter: 410, loss: 0.020717990080238474, w_module: 24.55206501387731\n",
      "iter: 420, loss: 0.02068310149693014, w_module: 24.5578063010722\n",
      "iter: 430, loss: 0.020648354993147978, w_module: 24.56353750501397\n",
      "iter: 440, loss: 0.020613745684786453, w_module: 24.569258667373163\n",
      "iter: 450, loss: 0.020579269874231337, w_module: 24.57496982864022\n",
      "iter: 460, loss: 0.020544924713914392, w_module: 24.580671028362445\n",
      "iter: 470, loss: 0.02051070796588358, w_module: 24.586362305319643\n",
      "iter: 480, loss: 0.020476617829969068, w_module: 24.592043697655264\n",
      "iter: 490, loss: 0.020442652820912077, w_module: 24.597715242974857\n",
      "iter: 500, loss: 0.020408811680479616, w_module: 24.603376978420606\n",
      "iter: 510, loss: 0.020375093314588717, w_module: 24.609028940728198\n",
      "iter: 520, loss: 0.02034149674829446, w_module: 24.61467116627059\n",
      "iter: 530, loss: 0.020308021093526512, w_module: 24.62030369109198\n",
      "iter: 540, loss: 0.02027466552601432, w_module: 24.62592655093454\n",
      "iter: 550, loss: 0.02024142926868082, w_module: 24.63153978125951\n",
      "iter: 560, loss: 0.02020831157976387, w_module: 24.6371434172642\n",
      "iter: 570, loss: 0.02017531174426657, w_module: 24.6427374938957\n",
      "iter: 580, loss: 0.020142429067813796, w_module: 24.648322045862177\n",
      "iter: 590, loss: 0.020109662872222836, w_module: 24.653897107642134\n",
      "iter: 600, loss: 0.020077012492307356, w_module: 24.65946271349228\n",
      "iter: 610, loss: 0.020044477273584264, w_module: 24.665018897454164\n",
      "iter: 620, loss: 0.020012056570580042, w_module: 24.67056569335979\n",
      "iter: 630, loss: 0.019979749745633726, w_module: 24.67610313483656\n",
      "iter: 640, loss: 0.019947556168001836, w_module: 24.681631255311558\n",
      "iter: 650, loss: 0.019915475213204258, w_module: 24.687150088015336\n",
      "iter: 660, loss: 0.019883506262551164, w_module: 24.692659665985296\n",
      "iter: 670, loss: 0.019851648702767233, w_module: 24.698160022068723\n",
      "iter: 680, loss: 0.01981990192574347, w_module: 24.703651188925512\n",
      "iter: 690, loss: 0.0197882653283142, w_module: 24.709133199030724\n",
      "iter: 700, loss: 0.019756738312093215, w_module: 24.7146060846769\n",
      "iter: 710, loss: 0.019725320283359188, w_module: 24.72006987797618\n",
      "iter: 720, loss: 0.01969401065294469, w_module: 24.725524610862344\n",
      "iter: 730, loss: 0.01966280883614278, w_module: 24.730970315092712\n",
      "iter: 740, loss: 0.019631714252651006, w_module: 24.736407022249892\n",
      "iter: 750, loss: 0.019600726326489258, w_module: 24.741834763743515\n",
      "iter: 760, loss: 0.019569844485956098, w_module: 24.74725357081184\n",
      "iter: 770, loss: 0.01953906816357134, w_module: 24.752663474523295\n",
      "iter: 780, loss: 0.019508396796026166, w_module: 24.758064505778037\n",
      "iter: 790, loss: 0.019477829824145764, w_module: 24.763456695309284\n",
      "iter: 800, loss: 0.019447366692833288, w_module: 24.768840073684835\n",
      "iter: 810, loss: 0.019417006851041942, w_module: 24.77421467130836\n",
      "iter: 820, loss: 0.01938674975173213, w_module: 24.779580518420733\n",
      "iter: 830, loss: 0.019356594851826305, w_module: 24.784937645101365\n",
      "iter: 840, loss: 0.01932654161217697, w_module: 24.790286081269418\n",
      "iter: 850, loss: 0.019296589497525655, w_module: 24.79562585668508\n",
      "iter: 860, loss: 0.019266737976471875, w_module: 24.80095700095076\n",
      "iter: 870, loss: 0.019236986521430407, w_module: 24.80627954351231\n",
      "iter: 880, loss: 0.01920733460860971, w_module: 24.81159351366017\n",
      "iter: 890, loss: 0.019177781717954097, w_module: 24.816898940530557\n",
      "iter: 900, loss: 0.019148327333132827, w_module: 24.82219585310656\n",
      "iter: 910, loss: 0.019118970941492473, w_module: 24.827484280219334\n",
      "iter: 920, loss: 0.01908971203402789, w_module: 24.832764250549126\n",
      "iter: 930, loss: 0.019060550105348788, w_module: 24.83803579262645\n",
      "iter: 940, loss: 0.019031484653641538, w_module: 24.843298934833108\n",
      "iter: 950, loss: 0.019002515180651137, w_module: 24.848553705403305\n",
      "iter: 960, loss: 0.018973641191635752, w_module: 24.853800132424666\n",
      "iter: 970, loss: 0.01894486219533872, w_module: 24.859038243839322\n",
      "iter: 980, loss: 0.018916177703958363, w_module: 24.864268067444918\n",
      "iter: 990, loss: 0.018887587233115267, w_module: 24.86948963089564\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object \n",
    "LR = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data without regularization (reg = 0)\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述代码，你会发现，在不考虑正则化的情况下，$||w||$ 随着训练次数的增加会不断增大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练完成后，你可以利用训练得到的分类器来进行预测。请你编写代码，计算训练集和测试集中的预测准确率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 1.0\n",
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "\n",
    "# training accuracy\n",
    "\n",
    "# TODO: compute the y_pred using LR.predict() function\n",
    "_,y_train_pred = LR.predict(x_train)\n",
    "\n",
    "# TODO: compute the accuracy\n",
    "train_acc = np.sum(y_train == y_train_pred)/len(y_train)\n",
    "\n",
    "print(\"Train accuracy: {}\".format(train_acc))\n",
    "\n",
    "\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "_,y_test_pred = LR.predict(x_test)\n",
    "test_acc = np.sum(y_test == y_test_pred)/len(y_test)\n",
    "print(\"Test accuracy: {}\".format(test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter: 0, loss: 152.45451735975882, w_module: 11.32850084425441\n",
      "iter: 10, loss: 15.229012221168249, w_module: 17.17919315545479\n",
      "iter: 20, loss: 12.717067631116146, w_module: 15.61201246864882\n",
      "iter: 30, loss: 10.806138783472313, w_module: 14.246365049809006\n",
      "iter: 40, loss: 9.419316859329856, w_module: 13.086407315557901\n",
      "iter: 50, loss: 8.480656088343302, w_module: 12.136250033768029\n",
      "iter: 60, loss: 7.899792694049427, w_module: 11.393157726481482\n",
      "iter: 70, loss: 7.575200935924059, w_module: 10.841599358152878\n",
      "iter: 80, loss: 7.411459160029551, w_module: 10.452937163736346\n",
      "iter: 90, loss: 7.336022845908453, w_module: 10.191309999841137\n",
      "iter: 100, loss: 7.303678380295569, w_module: 10.02146259646384\n",
      "iter: 110, loss: 7.290506041904443, w_module: 9.914067060190575\n",
      "iter: 120, loss: 7.28531856668592, w_module: 9.847373376301473\n",
      "iter: 130, loss: 7.283314996444742, w_module: 9.806442507988578\n",
      "iter: 140, loss: 7.2825480083027445, w_module: 9.781511476891874\n",
      "iter: 150, loss: 7.282254694905058, w_module: 9.76639792906735\n",
      "iter: 160, loss: 7.282141951447904, w_module: 9.757263130927923\n",
      "iter: 170, loss: 7.282098174993593, w_module: 9.751752318259454\n",
      "iter: 180, loss: 7.282080931563368, w_module: 9.748431777230397\n",
      "iter: 190, loss: 7.282074016545706, w_module: 9.746432583231735\n",
      "iter: 200, loss: 7.282071185382961, w_module: 9.745229603546836\n",
      "iter: 210, loss: 7.282069999801645, w_module: 9.744506036904548\n",
      "iter: 220, loss: 7.282069491651789, w_module: 9.744070978890512\n",
      "iter: 230, loss: 7.282069268840859, w_module: 9.743809475328968\n",
      "iter: 240, loss: 7.282069169046064, w_module: 9.743652340993687\n",
      "iter: 250, loss: 7.282069123492368, w_module: 9.743557952264636\n",
      "iter: 260, loss: 7.282069102356214, w_module: 9.743501274807075\n",
      "iter: 270, loss: 7.2820690924154245, w_module: 9.743467255824804\n",
      "iter: 280, loss: 7.282069087688463, w_module: 9.743446846592075\n",
      "iter: 290, loss: 7.2820690854211385, w_module: 9.743434609057992\n",
      "iter: 300, loss: 7.282069084326239, w_module: 9.74342727602571\n",
      "iter: 310, loss: 7.282069083794767, w_module: 9.743422885178305\n",
      "iter: 320, loss: 7.28206908353577, w_module: 9.743420258351765\n",
      "iter: 330, loss: 7.282069083409187, w_module: 9.743418688477893\n",
      "iter: 340, loss: 7.282069083347183, w_module: 9.743417751419084\n",
      "iter: 350, loss: 7.28206908331676, w_module: 9.7434171928982\n",
      "iter: 360, loss: 7.282069083301814, w_module: 9.74341686057271\n",
      "iter: 370, loss: 7.282069083294467, w_module: 9.743416663241831\n",
      "iter: 380, loss: 7.2820690832908515, w_module: 9.743416546357414\n",
      "iter: 390, loss: 7.2820690832890715, w_module: 9.743416477328703\n",
      "iter: 400, loss: 7.2820690832881985, w_module: 9.743416436708522\n",
      "iter: 410, loss: 7.282069083287763, w_module: 9.743416412910078\n",
      "iter: 420, loss: 7.282069083287553, w_module: 9.74341639904223\n",
      "iter: 430, loss: 7.282069083287448, w_module: 9.743416391015323\n",
      "iter: 440, loss: 7.282069083287395, w_module: 9.743416386408514\n",
      "iter: 450, loss: 7.28206908328737, w_module: 9.743416383793242\n",
      "iter: 460, loss: 7.282069083287358, w_module: 9.743416382329668\n",
      "iter: 470, loss: 7.282069083287351, w_module: 9.743416381526316\n",
      "iter: 480, loss: 7.282069083287348, w_module: 9.743416381097221\n",
      "iter: 490, loss: 7.282069083287346, w_module: 9.74341638087716\n",
      "iter: 500, loss: 7.282069083287346, w_module: 9.74341638077152\n",
      "iter: 510, loss: 7.282069083287347, w_module: 9.743416380726757\n",
      "iter: 520, loss: 7.282069083287347, w_module: 9.743416380713024\n",
      "iter: 530, loss: 7.282069083287345, w_module: 9.743416380714033\n",
      "iter: 540, loss: 7.282069083287345, w_module: 9.74341638072115\n",
      "iter: 550, loss: 7.282069083287345, w_module: 9.743416380730002\n",
      "iter: 560, loss: 7.282069083287347, w_module: 9.743416380738537\n",
      "iter: 570, loss: 7.282069083287344, w_module: 9.743416380745927\n",
      "iter: 580, loss: 7.282069083287347, w_module: 9.743416380751956\n",
      "iter: 590, loss: 7.282069083287345, w_module: 9.743416380756697\n",
      "iter: 600, loss: 7.282069083287345, w_module: 9.743416380760326\n",
      "iter: 610, loss: 7.282069083287345, w_module: 9.743416380763056\n",
      "iter: 620, loss: 7.282069083287347, w_module: 9.743416380765085\n",
      "iter: 630, loss: 7.282069083287344, w_module: 9.743416380766575\n",
      "iter: 640, loss: 7.282069083287345, w_module: 9.743416380767663\n",
      "iter: 650, loss: 7.282069083287347, w_module: 9.74341638076845\n",
      "iter: 660, loss: 7.282069083287347, w_module: 9.743416380769016\n",
      "iter: 670, loss: 7.282069083287345, w_module: 9.743416380769423\n",
      "iter: 680, loss: 7.282069083287345, w_module: 9.743416380769714\n",
      "iter: 690, loss: 7.282069083287344, w_module: 9.74341638076992\n",
      "iter: 700, loss: 7.282069083287345, w_module: 9.743416380770068\n",
      "iter: 710, loss: 7.282069083287344, w_module: 9.743416380770173\n",
      "iter: 720, loss: 7.282069083287346, w_module: 9.743416380770245\n",
      "iter: 730, loss: 7.282069083287347, w_module: 9.743416380770299\n",
      "iter: 740, loss: 7.282069083287346, w_module: 9.743416380770336\n",
      "iter: 750, loss: 7.282069083287345, w_module: 9.743416380770364\n",
      "iter: 760, loss: 7.282069083287344, w_module: 9.743416380770382\n",
      "iter: 770, loss: 7.282069083287347, w_module: 9.743416380770398\n",
      "iter: 780, loss: 7.282069083287344, w_module: 9.743416380770407\n",
      "iter: 790, loss: 7.282069083287345, w_module: 9.743416380770414\n",
      "iter: 800, loss: 7.282069083287348, w_module: 9.743416380770418\n",
      "iter: 810, loss: 7.282069083287345, w_module: 9.743416380770418\n",
      "iter: 820, loss: 7.282069083287347, w_module: 9.743416380770418\n",
      "iter: 830, loss: 7.282069083287344, w_module: 9.743416380770416\n",
      "iter: 840, loss: 7.282069083287345, w_module: 9.743416380770416\n",
      "iter: 850, loss: 7.282069083287344, w_module: 9.743416380770416\n",
      "iter: 860, loss: 7.282069083287346, w_module: 9.743416380770416\n",
      "iter: 870, loss: 7.282069083287344, w_module: 9.743416380770416\n",
      "iter: 880, loss: 7.282069083287345, w_module: 9.743416380770416\n",
      "iter: 890, loss: 7.282069083287347, w_module: 9.743416380770416\n",
      "iter: 900, loss: 7.282069083287347, w_module: 9.743416380770416\n",
      "iter: 910, loss: 7.282069083287346, w_module: 9.743416380770418\n",
      "iter: 920, loss: 7.282069083287346, w_module: 9.743416380770418\n",
      "iter: 930, loss: 7.282069083287345, w_module: 9.743416380770418\n",
      "iter: 940, loss: 7.282069083287345, w_module: 9.743416380770418\n",
      "iter: 950, loss: 7.282069083287347, w_module: 9.743416380770418\n",
      "iter: 960, loss: 7.282069083287347, w_module: 9.743416380770418\n",
      "iter: 970, loss: 7.282069083287347, w_module: 9.743416380770418\n",
      "iter: 980, loss: 7.282069083287347, w_module: 9.743416380770418\n",
      "iter: 990, loss: 7.282069083287347, w_module: 9.743416380770418\n"
     ]
    }
   ],
   "source": [
    "from logistic import LogisticRegression\n",
    "\n",
    "# create a LogisticRegression object and train it when using regularization\n",
    "LR = LogisticRegression()\n",
    "LR.fit(x_train, y_train, lr=0.1, n_iter=1000,reg=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "# TODO: Implement the code to compute the accuracy of logistic regression (LR) in the test set. Note that LR itself is already trained, if you have run the above code.\n",
    "# TODO: test accuracy, proceed similarly as above\n",
    "_,y_test_pred = LR.predict(x_test)\n",
    "test_acc = np.sum(y_test == y_test_pred)/len(y_test)\n",
    "print(\"Test accuracy: {}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "运行上述带有正则化的代码后，请观察 $||w||$ 的变化，并讨论正则化的实际意义。(请将答案写在下方)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在正则化之后，||W||先增大后减小，随后稳定在一个值的附近，正则化的实际意义在与，将模型的参数控制在一个相对稳定层面，防止了过度拟合，提升了模型的泛化能力。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
